## SGD(확률적 경사 하강법)
- $\gamma$: 학습률
$$W \leftarrow W - \gamma\frac{\partial L}{\partial W}$$
- 전체를 한 번에 계산하지 않고, 확률적으로 일부 샘플을 뽑아 조금씩 나누어 학습을 하는 과정
- 반복할 때마다 다루는 데이터의 수가 적기 때문에 한 번 처리하는 속도는 빠름
- 확률적이기 때문에, 배치 경사하강법보다 불안정
- 손실함수의 최솟값에 이를 때까지 다소 위아래롤 요동치면서 이동
- 위와 같은 문제 때무넹 미니 배치 경사하강법(mini-batch gradient descent)로 학습을 진행


## Momentum
- $\alpha$: 관성계수
- $v$: 속도
$$v \leftarrow \alpha v - \gamma\frac{\partial L}{\partial W}$$
$$W \leftarrow W + v$$
- 운동량을 의미, 관선과 관련
- 공이 그릇의 경사면을 따라서 내려가는 듯한 모습
- 이전의 속도를 유지하려는 성향
  - 경사 하강을 좀 더 유지하려는 성격을 지님
- 단순히 SGD만 사용하는 것보다 적게 방향이 변함
  <img width="800" alt="image" src="https://github.com/y100861/deep-learning-from-scratch-master/assets/107607076/d170bf4b-9bf2-4c7c-a554-0ead3d837b04">


## AdaGrad
- $h$: 기존 기울기를 제곱하여 더한 값
$$h \leftarrow h + \frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W}$$
$$W \leftarrow - \gamma \frac{1}{\sqrt{h}} \frac{\partial L}{\partial W}$$
- 가장 가파른 경사를 따라 빠르게 하강하는 방법
- 적응적 학습률이라고도 하며 학습률을 변화시키며 진행
- 경사가 급할 때는 빠르게 변화, 완만할 때는 느리게 변화
- 간단한 문제에서는 좋을 수는 있지만 딥러닝에서는 자주 쓰이지 않음
  - 학습률이 너무 감소되어 global minimum에 도달하기 전에 학습이 빨리 종료될 수 있기 때문
