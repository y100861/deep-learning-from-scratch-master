# 매개변수 갱신

## SGD(확률적 경사 하강법, Stochastic Gradient Descent)
- $\eta$: 학습률
$$W \leftarrow W - \eta\frac{\partial L}{\partial W}$$
- 전체를 한 번에 계산하지 않고, 확률적으로 일부 샘플을 뽑아 조금씩 나누어 학습을 하는 과정
- 반복할 때마다 다루는 데이터의 수가 적기 때문에 한 번 처리하는 속도는 빠름
- 확률적이기 때문에, 배치 경사하강법보다 불안정
- 손실함수의 최솟값에 이를 때까지 다소 위아래롤 요동치면서 이동
- 위와 같은 문제 때무넹 미니 배치 경사하강법(mini-batch gradient descent)로 학습을 진행


## Momentum
- $\alpha$: 관성계수
- $v$: 속도
$$v \leftarrow \alpha v - \eta\frac{\partial L}{\partial W}$$
$$W \leftarrow W + v$$
- 운동량을 의미, 관선과 관련
- 공이 그릇의 경사면을 따라서 내려가는 듯한 모습
- 이전의 속도를 유지하려는 성향
  - 경사 하강을 좀 더 유지하려는 성격을 지님
- 단순히 SGD만 사용하는 것보다 적게 방향이 변함
  <img width="800" alt="image" src="https://github.com/y100861/deep-learning-from-scratch-master/assets/107607076/d170bf4b-9bf2-4c7c-a554-0ead3d837b04">


## AdaGrad(Adaptive Gradient)
- $h$: 기존 기울기를 제곱하여 더한 값
$$h \leftarrow h + \frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W}$$
$$W \leftarrow - \eta \frac{1}{\sqrt{h}} \frac{\partial L}{\partial W}$$
- 가장 가파른 경사를 따라 빠르게 하강하는 방법
- 적응적 학습률이라고도 하며 학습률을 변화시키며 진행
- 경사가 급할 때는 빠르게 변화, 완만할 때는 느리게 변화
- 간단한 문제에서는 좋을 수는 있지만 딥러닝에서는 자주 쓰이지 않음
  - 학습률이 너무 감소되어 global minimum에 도달하기 전에 학습이 빨리 종료될 수 있기 때문
- 과거의 기울기를 제곱하여 계속 더해가기 때문에 학습을 진행할수록 갱신 강도가 약해짐($\because \frac{1}{\sqrt h}$)


## RMSProp(Root Mean Square Propagation)
- $h$: 기존 기울기를 제곱하여 업데이트 계수를 곱한 값과 업데이트 계수를 곱한 값을 더해줌
- $\rho$: 지수 평균의 업데이트 계수
$$h \leftarrow \rho h + (1 - \rho)\frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W}$$
$$W \leftarrow W - \frac{\eta \frac{\partial L}{\partial W}}{\sqrt{h} + \epsilon}$$
- AdaGrad를 보완하기 위한 방법으로 등장
- 합 대신 지수의 평균값을 활용
- 학습이 안 되기 시작하면 학습률이 커져 잘 되게끔하고, 학습률이 너무 크면 학습률을 다시 줄임


## Adam(Adaptive Moment Estimation)
- $\beta$: 지수 평균의 업데이트 계수
- $\beta_1 \approx 0.9, \beta2 \approx 0.999$
$$t \leftarrow t + 1$$
$$m_t \leftarrow \beta_1m_{t-1} + (1 - \beta_1)\frac{\partial L}{\partial W}$$
$$v_t \leftarrow \beta_2v_{t-1} + (1 - \beta_2)\frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W}$$
$$\eta \leftarrow \frac{\eta \sqrt{1 - \beta_2^t}}{1 - \beta_1^t}$$
$$\hat{m_t} \leftarrow \frac{m_t}{1-\beta^t_1}$$
$$\hat{v_t} \leftarrow \frac{v_t}{1-\beta^t_2}$$
$$W_t \leftarrow W_{t-1} - \frac{\eta\hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon}$$
- Momentum 최적화와 RMSProp의 아이디어를 합친 것
- 지난 그래디언트의 지수 감소 평균을 따르고(Momentum), 지난 그래디언트 **제곱의** 지수 감소 평균(RMSProp)을 따름
- 가장 많이 사용되는 최적화 방법


# 가중치 초깃값 설정

## 초깃값: 0(Zeros)
- 학습이 올바르게 진행되지 않음
- 오차역전파법에서 모든 가중치의 값이 똑같이 갱신됨
  <img width="800" alt="image" src=https://github.com/y100861/deep-learning-from-scratch-master/assets/107607076/ac30f728-b8e4-4de0-b8d8-5f83a54c0b35>


## 초깃값: 균일분포(Uniform)
- 활성화 값이 균일하지 않음(활성화함수: Sigmoid)
- 역전파로 전해지는 기울기값이 사라짐
 <img width="800" alt="image" src=https://github.com/y100861/deep-learning-from-scratch-master/assets/107607076/e4bf190c-4286-461a-a371-56a28fca963a>


## 초깃값: 정규분포(Normalization)
- 활성화함수를 통과하면 양쪽으로 퍼짐
- 0과 1에 퍼지면서 기울기 소실문제 발생
<img width="800" alt="image" src=https://github.com/y100861/deep-learning-from-scratch-master/assets/107607076/1ced6fcb-1ba4-4bf0-8792-4f5cb99b59fd>
