## SGD(확률적 경사 하강법, Stochastic Gradient Descent)
- $\eta$: 학습률
$$W \leftarrow W - \eta\frac{\partial L}{\partial W}$$
- 전체를 한 번에 계산하지 않고, 확률적으로 일부 샘플을 뽑아 조금씩 나누어 학습을 하는 과정
- 반복할 때마다 다루는 데이터의 수가 적기 때문에 한 번 처리하는 속도는 빠름
- 확률적이기 때문에, 배치 경사하강법보다 불안정
- 손실함수의 최솟값에 이를 때까지 다소 위아래롤 요동치면서 이동
- 위와 같은 문제 때무넹 미니 배치 경사하강법(mini-batch gradient descent)로 학습을 진행


## Momentum
- $\alpha$: 관성계수
- $v$: 속도
$$v \leftarrow \alpha v - \eta\frac{\partial L}{\partial W}$$
$$W \leftarrow W + v$$
- 운동량을 의미, 관선과 관련
- 공이 그릇의 경사면을 따라서 내려가는 듯한 모습
- 이전의 속도를 유지하려는 성향
  - 경사 하강을 좀 더 유지하려는 성격을 지님
- 단순히 SGD만 사용하는 것보다 적게 방향이 변함
  <img width="800" alt="image" src="https://github.com/y100861/deep-learning-from-scratch-master/assets/107607076/d170bf4b-9bf2-4c7c-a554-0ead3d837b04">


## AdaGrad(Adaptive Gradient)
- $h$: 기존 기울기를 제곱하여 더한 값
$$h \leftarrow h + \frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W}$$
$$W \leftarrow - \eta \frac{1}{\sqrt{h}} \frac{\partial L}{\partial W}$$
- 가장 가파른 경사를 따라 빠르게 하강하는 방법
- 적응적 학습률이라고도 하며 학습률을 변화시키며 진행
- 경사가 급할 때는 빠르게 변화, 완만할 때는 느리게 변화
- 간단한 문제에서는 좋을 수는 있지만 딥러닝에서는 자주 쓰이지 않음
  - 학습률이 너무 감소되어 global minimum에 도달하기 전에 학습이 빨리 종료될 수 있기 때문
- 과거의 기울기를 제곱하여 계속 더해가기 때문에 학습을 진행할수록 갱신 강도가 약해짐($\because \frac{1}{\sqrt h}$)


## RMSProp(Root Mean Square Propagation)
- $h$: 기존 기울기를 제곱하여 업데이트 계수를 곱한 값과 업데이트 계수를 곱한 값을 더해줌
- $\rho$: 지수 평균의 업데이트 계수
$$h \leftarrow \rho h + (1 - \rho)\frac{\partial L}{\partial W} \odot \frac{\partial L}{\partial W}$$
$$W \leftarrow W - \frac{\eta \frac{\partial L}{\partial W}}{\sqrt{h} + \epsilon}$$
- AdaGrad를 보완하기 위한 방법으로 등장
- 합 대신 지수의 평균값을 활용
- 학습이 안 되기 시작하면 학습률이 커져 잘 되게끔하고, 학습률이 너무 크면 학습률을 다시 줄임


## Adam(Adaptive Moment Estimation)
