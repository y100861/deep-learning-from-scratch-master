## SGD(확률적 경사 하강법)
- $\gamma$: 학습률
$$W \leftarrow W - \gamma\frac{\partial L}{\partial W}$$
- 전체를 한 번에 계산하지 않고, 확률적으로 일부 샘플을 뽑아 조금씩 나누어 학습을 하는 과정
- 반복할 때마다 다루는 데이터의 수가 적기 때문에 한 번 처리하는 속도는 빠름
- 확률적이기 때문에, 배치 경사하강법보다 불안정
- 손실함수의 최솟값에 이를 때까지 다소 위아래롤 요동치면서 이동
- 위와 같은 문제 때무넹 미니 배치 경사하강법(mini-batch gradient descent)로 학습을 진행


## Momentum
- $\alpha$: 관성계수
- $v$: 속도
$$v \leftarrow \alpha v - \gamma\frac{\partial L}{\partial W}$$
$$W \leftarrow W + v$$
- 운동량을 의미, 관선과 관련
- 공이 그릇의 경사면을 따라서 내려가는 듯한 모습
- 이전의 속도를 유지하려는 성향
  - 경사 하강을 좀 더 유지하려는 성격을 지님
- 단순히 SGD만 사용하는 것보다 적게 방향이 변함
  <img width="800" alt="image" src="https://github.com/y100861/deep-learning-from-scratch-master/assets/107607076/d170bf4b-9bf2-4c7c-a554-0ead3d837b04">

